{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/venkyroyal/nandu_practise/TextSummarization/research'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/venkyroyal/nandu_practise/TextSummarization'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "          config = self.config.model_trainer\n",
    "          params = self.params.TrainingArguments\n",
    "\n",
    "          create_directories([config.root_dir])\n",
    "\n",
    "          model_trainer_config = ModelTrainerConfig(\n",
    "                root_dir= config.root_dir,\n",
    "                data_path= config.data_path,\n",
    "                model_ckpt= config.model_ckpt,\n",
    "                num_train_epochs= params.num_train_epochs,\n",
    "                warmup_steps = params.warmup_steps,\n",
    "                per_device_train_batch_size= params.per_device_train_batch_size,\n",
    "                weight_decay= params.weight_decay,\n",
    "                logging_steps= params.logging_steps,\n",
    "                evaluation_strategy= params.evaluation_strategy,\n",
    "                eval_steps= params.eval_steps,\n",
    "                save_steps= params.save_steps,\n",
    "                gradient_accumulation_steps= params.gradient_accumulation_steps\n",
    "\n",
    "          )\n",
    "\n",
    "          return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model_pegasus)\n",
    "\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir= self.config.root_dir, num_train_epochs = self.config.num_train_epochs, warmup_steps= self.config.warmup_steps,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size= self.config.per_device_train_batch_size,\n",
    "            weight_decay= self.config.weight_decay, logging_steps= self.config.logging_steps,\n",
    "            evaluation_strategy= self.config.evaluation_strategy, eval_steps= self.config.eval_steps, save_steps= 1e6,\n",
    "            gradient_accumulation_steps= self.config.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(model = model_pegasus, args = trainer_args,\n",
    "                          tokenizer= tokenizer, data_collator= seq2seq_data_collator,\n",
    "                          train_dataset= dataset_samsum_pt['test'],\n",
    "                          eval_dataset= dataset_samsum_pt['validation'])\n",
    "        \n",
    "        trainer.train()\n",
    "\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir, \"pegasus-samsum-model\"))\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-19 19:07:46,592: INFO: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-19 19:07:46,607: INFO: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-19 19:07:46,608: INFO: created directory at: artifacts]\n",
      "[2024-01-19 19:07:46,612: INFO: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [122:20:38<?, ?it/s]\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.87 GB, other allocations: 2.00 GB, max allowed: 9.07 GB). Tried to allocate 375.40 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     model_trainer_config\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_trainer_config()\n\u001b[1;32m      4\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m ModelTrainer(config \u001b[39m=\u001b[39m model_trainer_config)\n\u001b[0;32m----> 5\u001b[0m     model_trainer_config\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset_samsum_pt \u001b[39m=\u001b[39m load_from_disk(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_path)\n\u001b[1;32m     14\u001b[0m trainer_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     15\u001b[0m     output_dir\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mroot_dir, num_train_epochs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_train_epochs, warmup_steps\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mwarmup_steps,\n\u001b[1;32m     16\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mper_device_train_batch_size, per_device_eval_batch_size\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mper_device_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model \u001b[39m=\u001b[39;49m model_pegasus, args \u001b[39m=\u001b[39;49m trainer_args,\n\u001b[1;32m     23\u001b[0m                   tokenizer\u001b[39m=\u001b[39;49m tokenizer, data_collator\u001b[39m=\u001b[39;49m seq2seq_data_collator,\n\u001b[1;32m     24\u001b[0m                   train_dataset\u001b[39m=\u001b[39;49m dataset_samsum_pt[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     25\u001b[0m                   eval_dataset\u001b[39m=\u001b[39;49m dataset_samsum_pt[\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     27\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     29\u001b[0m model_pegasus\u001b[39m.\u001b[39msave_pretrained(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mroot_dir, \u001b[39m\"\u001b[39m\u001b[39mpegasus-samsum-model\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/transformers/trainer.py:456\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device\n\u001b[1;32m    454\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    455\u001b[0m ):\n\u001b[0;32m--> 456\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    458\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/transformers/trainer.py:690\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 690\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    691\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/transformers/modeling_utils.py:2460\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2456\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2457\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2459\u001b[0m         )\n\u001b[0;32m-> 2460\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregister_full_backward_pre_hook\u001b[39m(\n\u001b[1;32m   1155\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1156\u001b[0m     hook: Callable[[\u001b[39m\"\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m\"\u001b[39m, _grad_t], Union[\u001b[39mNone\u001b[39;00m, _grad_t]],\n\u001b[1;32m   1157\u001b[0m     prepend: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1159\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Register a backward pre-hook on the module.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m \n\u001b[1;32m   1161\u001b[0m \u001b[39m    The hook will be called every time the gradients for the module are computed.\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \u001b[39m    The hook should have the following signature::\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \n\u001b[1;32m   1164\u001b[0m \u001b[39m        hook(module, grad_output) -> tuple[Tensor] or None\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \n\u001b[1;32m   1166\u001b[0m \u001b[39m    The :attr:`grad_output` is a tuple. The hook should\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[39m    not modify its arguments, but it can optionally return a new gradient with\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[39m    respect to the output that will be used in place of :attr:`grad_output` in\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[39m    subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[39m    all non-Tensor arguments.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \n\u001b[1;32m   1172\u001b[0m \u001b[39m    For technical reasons, when this hook is applied to a Module, its forward function will\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[39m    receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[39m    of each Tensor returned by the Module's forward function.\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \n\u001b[1;32m   1176\u001b[0m \u001b[39m    .. warning ::\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39m        Modifying inputs inplace is not allowed when using backward hooks and\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[39m        will raise an error.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39m        hook (Callable): The user-defined hook to be registered.\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m        prepend (bool): If true, the provided ``hook`` will be fired before\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m            all existing ``backward_pre`` hooks on this\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39m            :class:`torch.nn.modules.Module`. Otherwise, the provided\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[39m            ``hook`` will be fired after all existing ``backward_pre`` hooks\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[39m            on this :class:`torch.nn.modules.Module`. Note that global\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m            ``backward_pre`` hooks registered with\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39m            :func:`register_module_full_backward_pre_hook` will fire before\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m            all hooks registered by this method.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[1;32m   1191\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \n\u001b[1;32m   1196\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     handle \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mRemovableHandle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks)\n\u001b[1;32m   1198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks[handle\u001b[39m.\u001b[39mid] \u001b[39m=\u001b[39m hook\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m         module._apply(fn)\n\u001b[1;32m    804\u001b[0m def compute_should_use_set_data(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         # If the new tensor has compatible tensor type as the existing tensor,\n\u001b[1;32m    807\u001b[0m         # the current behavior is to change the tensor in-place using `.data =`,\n\u001b[1;32m    808\u001b[0m         # and the future behavior is to overwrite the existing tensor. However,\n\u001b[1;32m    809\u001b[0m         # changing the current behavior is a BC-breaking change, and we want it\n\u001b[0;32m--> 810\u001b[0m         # to happen in future releases. So for now we introduce the\n\u001b[1;32m    811\u001b[0m         # `torch.__future__.get_overwrite_module_params_on_conversion()`\n\u001b[1;32m    812\u001b[0m         # global flag to let the user control whether they want the future\n\u001b[1;32m    813\u001b[0m         # behavior of overwriting the existing tensor or not.\n\u001b[1;32m    814\u001b[0m         return not torch.__future__.get_overwrite_module_params_on_conversion()\n\u001b[1;32m    815\u001b[0m     else:\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m         module._apply(fn)\n\u001b[1;32m    804\u001b[0m def compute_should_use_set_data(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         # If the new tensor has compatible tensor type as the existing tensor,\n\u001b[1;32m    807\u001b[0m         # the current behavior is to change the tensor in-place using `.data =`,\n\u001b[1;32m    808\u001b[0m         # and the future behavior is to overwrite the existing tensor. However,\n\u001b[1;32m    809\u001b[0m         # changing the current behavior is a BC-breaking change, and we want it\n\u001b[0;32m--> 810\u001b[0m         # to happen in future releases. So for now we introduce the\n\u001b[1;32m    811\u001b[0m         # `torch.__future__.get_overwrite_module_params_on_conversion()`\n\u001b[1;32m    812\u001b[0m         # global flag to let the user control whether they want the future\n\u001b[1;32m    813\u001b[0m         # behavior of overwriting the existing tensor or not.\n\u001b[1;32m    814\u001b[0m         return not torch.__future__.get_overwrite_module_params_on_conversion()\n\u001b[1;32m    815\u001b[0m     else:\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(param, Parameter)\n\u001b[1;32m    832\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_leaf\n\u001b[0;32m--> 833\u001b[0m     out_param \u001b[39m=\u001b[39m Parameter(param_applied, param\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters[key] \u001b[39m=\u001b[39m out_param\n\u001b[1;32m    836\u001b[0m \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/nandu_practise/TextSummarization/textS/lib/python3.9/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(convert)\n\u001b[1;32m   1154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregister_full_backward_pre_hook\u001b[39m(\n\u001b[1;32m   1155\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1156\u001b[0m     hook: Callable[[\u001b[39m\"\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m\"\u001b[39m, _grad_t], Union[\u001b[39mNone\u001b[39;00m, _grad_t]],\n\u001b[1;32m   1157\u001b[0m     prepend: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m-> 1158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1159\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Register a backward pre-hook on the module.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \n\u001b[1;32m   1161\u001b[0m \u001b[39m    The hook will be called every time the gradients for the module are computed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \n\u001b[1;32m   1196\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     handle \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mRemovableHandle(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 6.87 GB, other allocations: 2.00 GB, max allowed: 9.07 GB). Tried to allocate 375.40 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config = model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
